{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "294f5baa",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "Is a specialized type of neural network model designed for working with two-dimensional image data, although they can be used with one-dimensional and three-dimensional data.\n",
    "\n",
    "* the convolutional layer that gives the network its name involves a convolution which is a linear operation that involves the multiplication of a set of weights with the input.\n",
    "\n",
    "* the multiplication is performed between an array of input data and a two-dimensional array of weights, called a filter or a kernel.\n",
    "\n",
    "* The filter is smaller than the input data and the type of multiplication applied between a filter-sized patch of the input and the filter is a element wise dot product which is then summed, always resulting in a single value.\n",
    "\n",
    "* Using a filter smaller than the input is intentional as it allows the same filter (set of weights) to be multiplied by the input array multiple times at different points on the input\n",
    "\n",
    "* If the filter is designed to detect a specific type of feature in the input, then the application of that filter systematically across the entire input image allows the filter an opportunity to discover that feature anywhere in the image. This capability is commonly referred to as translation invariance.\n",
    "\n",
    "> Invariance to local translation can be a very useful property if we care more about whether some feature is present than exactly where it is. For example, when determining whether an image contains a face, we need not know the location of the eyes with pixel-perfect accuracy, we just need to know that there is an eye on the left side of the face and an eye on the right side of the face.\n",
    "\n",
    "* Once a feature map is created, we can pass each value in the feature map through a nonlinearity, such as a ReLU, much like we do for the outputs of a fully connected layer.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d834d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c642bbe9",
   "metadata": {},
   "source": [
    "## 1D Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4350a97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.asarray([0, 0, 0, 1, 1, 0, 0, 0])\n",
    "data=data.reshape(1,8,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eb73ea",
   "metadata": {},
   "source": [
    "The input to Keras must be three dimensional for a 1D convolutional layer.\n",
    "\n",
    "* The first dimension refers to each input sample; in this case, we only have one sample. \n",
    "* The second dimension refers to the length of each sample; in this case, the length is eight. \n",
    "* The third dimension refers to the number of channels in each sample; in this case, we only have a single channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a58a9df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1cf57c5daf0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(data.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b2bbf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model=tf.keras.Sequential([tf.keras.layers.Conv1D(1,3,activation=\"relu\",input_shape=(8,1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dba714",
   "metadata": {},
   "source": [
    "By default, the filters in a convolutional layer are initialized with random weights. But here we will manually specify the weights for the single filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "826d5fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector=np.asarray([0,1,0])\n",
    "weights=[np.asarray(detector.reshape(-1,1,1)),np.asarray([0.0])]\n",
    "model.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7646e0e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[0.]],\n",
       " \n",
       "        [[1.]],\n",
       " \n",
       "        [[0.]]], dtype=float32),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf460a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]]]\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict(data)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd832df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1cf5896b2e0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1934f95",
   "metadata": {},
   "source": [
    "## 2D Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b2d22bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 6, 6, 1)           10        \n",
      "=================================================================\n",
      "Total params: 10\n",
      "Trainable params: 10\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "[array([[[[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]]],\n",
      "\n",
      "\n",
      "       [[[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]]],\n",
      "\n",
      "\n",
      "       [[[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]]]], dtype=float32), array([0.], dtype=float32)]\n",
      "[0.0, 0.0, 3.0, 3.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 3.0, 3.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 3.0, 3.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 3.0, 3.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 3.0, 3.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 3.0, 3.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# define input data\n",
    "data = [[0, 0, 0, 1, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 1, 0, 0, 0]]\n",
    "data = np.asarray(data)\n",
    "data = data.reshape(1, 8, 8, 1)\n",
    "# create model\n",
    "model1=tf.keras.Sequential([tf.keras.layers.Conv2D(1, (3,3), activation='relu', input_shape=(8, 8, 1))])\n",
    "print(model1.summary())\n",
    "\n",
    "# define a vertical line detector\n",
    "detector = [[[[0]],[[1]],[[0]]],\n",
    "            [[[0]],[[1]],[[0]]],\n",
    "            [[[0]],[[1]],[[0]]]]\n",
    "weights = [np.asarray(detector), np.asarray([0.0])]\n",
    "# store the weights in the model\n",
    "model1.set_weights(weights)\n",
    "# confirm they were stored\n",
    "print(model1.get_weights())\n",
    "# apply filter to input data\n",
    "y_pred1 = model1.predict(data)\n",
    "for r in range(y_pred1.shape[1]):\n",
    "    # print each column in the row\n",
    "    print([y_pred1[0,r,c,0] for c in range(y_pred1.shape[2])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "64c1c7fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1cf591727c0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(tf.squeeze(y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0f64cdec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 8, 8, 1), (1, 6, 6, 1))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape,y_pred1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ba370c",
   "metadata": {},
   "source": [
    "# Convolution Neural network\n",
    "The agenda for this field is to enable machines to view the world as humans do, perceive it in a similar manner and even use the knowledge for a multitude of tasks such as Image & Video recognition, Image Analysis & Classification, Media Recreation, Recommendation Systems, Natural Language Processing, etc. \n",
    "\n",
    "* The objective of the Convolution Operation is to extract the high-level features such as edges, from the input image. Convolutional layers in a convolutional neural network systematically apply learned filters to input images in order to create feature maps that summarize the presence of those features in the input\n",
    "\n",
    "* A limitation of the feature map output of convolutional layers is that they record the precise position of features in the input. This means that small movements in the position of the feature in the input image will result in a different feature map. This can happen with re-cropping, rotation, shifting, and other minor changes to the input image.\n",
    "\n",
    "* A common approach to addressing this problem from signal processing is called down sampling. This is where a lower resolution version of an input signal is created that still contains the large or important structural elements, without the fine detail that may not be as useful to the task.\n",
    "\n",
    "* Down sampling can be achieved with convolutional layers by changing the stride of the convolution across the image. A more robust and common approach is to use a pooling layer.\n",
    "\n",
    "* Pooling involves selecting a pooling operation, much like a filter to be applied to feature maps. The size of the pooling operation or filter is smaller than the size of the feature map; specifically, it is almost always 2×2 pixels applied with a stride of 2 pixels.\n",
    "\n",
    "* This means that the pooling layer will always reduce the size of each feature map by a factor of 2, e.g. each dimension is halved, reducing the number of pixels or values in each feature map to one quarter the size\n",
    "\n",
    ">The pooling operation is specified, rather than learned. Two common functions used in the pooling operation are:\n",
    "- ` Average Pooling: Calculate the average value for each patch on the feature map.`\n",
    "- ` Maximum Pooling (or Max Pooling): Calculate the maximum value for each patch of the feature map.`\n",
    "\n",
    "## Average Pooling Layer\n",
    "\n",
    "On two-dimensional feature maps, pooling is typically applied in 2×2 patches of the feature map with a stride of (2,2).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e74a976b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 6, 6, 1)           10        \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 3, 3, 1)           0         \n",
      "=================================================================\n",
      "Total params: 10\n",
      "Trainable params: 10\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "# we can add padding also\n",
    "model2= tf.keras.Sequential([tf.keras.layers.Conv2D(1,(3,3),activation=\"relu\",input_shape=(8,8,1)),\n",
    "                             tf.keras.layers.AveragePooling2D()])\n",
    "\n",
    "# summarize the model\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "861ec119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 3.0, 0.0]\n",
      "[0.0, 3.0, 0.0]\n",
      "[0.0, 3.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# define a vertical line detector\n",
    "detector = [[[[0]],[[1]],[[0]]],\n",
    "            [[[0]],[[1]],[[0]]],\n",
    "            [[[0]],[[1]],[[0]]]]\n",
    "weights = [np.asarray(detector), np.asarray([0.0])]\n",
    "# store the weights in the model\n",
    "model2.set_weights(weights)\n",
    "# apply filter to input data\n",
    "y_pred2 = model2.predict(data)\n",
    "# enumerate rows\n",
    "for r in range(y_pred2.shape[1]):\n",
    "    # print each column in the row\n",
    "    print([y_pred2[0,r,c,0] for c in range(y_pred2.shape[2])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "180ea8f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 8, 8, 1), (1, 3, 3, 1))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape, y_pred2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "db91261d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1cf591d6eb0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(tf.squeeze(y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6685b1a",
   "metadata": {},
   "source": [
    "y_pred2 generated from model 2 with average pooling has reduced size as compared to y_pred generated from model 1. The same thing is evident from model summary also\n",
    "\n",
    "## Max Pooling Layer\n",
    "\n",
    "Maximum pooling, or max pooling, is a pooling operation that calculates the maximum, or largest, value in each patch of each feature map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f336fb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 6, 6, 1)           10        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 3, 3, 1)           0         \n",
      "=================================================================\n",
      "Total params: 10\n",
      "Trainable params: 10\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "# we can add padding also\n",
    "model3= tf.keras.Sequential([tf.keras.layers.Conv2D(1,(3,3),activation=\"relu\",input_shape=(8,8,1)),\n",
    "                             tf.keras.layers.MaxPooling2D()])\n",
    "\n",
    "# summarize the model\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "06026a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 3.0, 0.0]\n",
      "[0.0, 3.0, 0.0]\n",
      "[0.0, 3.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# define a vertical line detector\n",
    "detector = [[[[0]],[[1]],[[0]]],\n",
    "            [[[0]],[[1]],[[0]]],\n",
    "            [[[0]],[[1]],[[0]]]]\n",
    "weights = [np.asarray(detector), np.asarray([0.0])]\n",
    "# store the weights in the model\n",
    "model3.set_weights(weights)\n",
    "# apply filter to input data\n",
    "y_pred3 = model3.predict(data)\n",
    "# enumerate rows\n",
    "for r in range(y_pred3.shape[1]):\n",
    "    # print each column in the row\n",
    "    print([y_pred3[0,r,c,0] for c in range(y_pred3.shape[2])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c2bb6b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1cf5a2ded30>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(tf.squeeze(y_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "db979ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 8, 8, 1), (1, 3, 3, 1))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape,y_pred3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e208274a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
